{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper is [here](https://arxiv.org/pdf/1605.07154v1.pdf)\n",
    "\n",
    "*Note*:\n",
    "It seems like no one has implemented this optimizer so far, so it would be \"novel\" if we could implement it. Also, implementation shouldn't be too hard. Anyone interested? \n",
    "\n",
    "# How to analyze the parameter-space geometry of recurrent neural networks (RNNs)\n",
    "\n",
    "Observations:\n",
    "- optimization is inherently tied to a choice of geometry\n",
    "- A choice of geometry is represented by a choice of complexity measure or “norm” (some measure of magnitude on functions, not on weights)\n",
    "## Answer\n",
    "Step1: By investigating which transofrmation is invariant to an objective function. (this invariant transformation can be found by using Theorem 1 in the paper) \n",
    "\n",
    "\"The above theorem shows that there are many transformations under which RNNs represent the same function....Therefore, we would like to have optimization algorithms that are invariant to these transformations and in order to do so, we need to look at measures that are invariant to such mappings.\"\n",
    "\n",
    "Step2: After finding such invariant transformation,  we want to find an optimization algorithm that are invariant to these transformation.\n",
    "\n",
    "Step3: In order to do so, we need to look at measure that are invariant to such mappings.\n",
    "\n",
    "Step4: That measure is \"path-norm\" \n",
    "\n",
    "# What is path-norm?\n",
    "(Read Section 4 in this paper Path-SGD: Path-Normalized Optimization in Deep Neural Networks: https://arxiv.org/pdf/1506.02617v1.pdf for details)\n",
    "\n",
    "## Path-Reguralizer\n",
    "Sum of the products of squared weights along the Path, where the Path is all pathes from input nodes to output nodes. \n",
    "Or equivalently, \n",
    "$L_p$ path-reguralizer is\n",
    "$l_p$ norm on the path vector $v$, where the path vector $v \\in R^n$, where $n$ is the number of paths from the input to output nodes, and $v_i$ = the product of weights from an input node to an output node. \n",
    "\n",
    "\n",
    "# What are invariance properties?\n",
    "Does this mean scale invariance? Or linear transformation invariance? \n",
    "Answer: It was a linear transformation on weights.\n",
    "\n",
    "# Importance of invariance on weights transformation in Neural Network Optimization\n",
    "\"Feedforward networks (with or without shared weights) are highly over-parameterized, i.e. there are many parameter settings $p$ that represent the same function $f_p$. Since our true object of interest is the function $f$, and not the identity $p$ of the parameters, it would be beneficial if optimization would depend only on $f_p$ and not get “distracted” by difference in $p$ that does not affect $f_p$. It is therefore helpful to study the transformations on the parameters that will not change the function presented by the network and come up with methods that their performance is not affected by such transformations.\n",
    "\n",
    "\n",
    "\n",
    "# Motivation \n",
    "\"Improving training RNNs with ReLU activations has been the subject of some recent attention, with most research focusing on different initialization strategies\" \n",
    "\n",
    "50min up to here. \n",
    "\n",
    "\"I wonder how this compares to batch normalization, which tends to enforce a per unit L2 regularization.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
