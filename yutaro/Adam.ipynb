{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are initialization bias correction terms (Sec 3 in the paper)\n",
    "(a.k.a How did you get $\\hat{m_t}$ and $\\hat{v_t}$)\n",
    "\n",
    "## Problem Statement\n",
    "------------------------------------------------------------\n",
    "Let $g$ be the gradient of the stochastic objective $f$,\n",
    "and $g_1, ..., g_T$ be the gradients at subsequent timesteps, each a draw from an\n",
    "underlying gradient distribution $g_t \\sim p(g_t)$. \n",
    "Let the initial moving average $v_0 = 0$ (a vector of zeros).\n",
    "Estimate its second\n",
    "raw moment (uncentered variance) using an exponential moving average of the squared gradient,\n",
    "with decay rate $\\beta_2$. \n",
    "\n",
    "----------------------------------------------------------------\n",
    "\n",
    "## Answer\n",
    "In order to find bias-corrected estimator for $g_t \\odot g_t$, we want to know how $E[v_t]$ is different from $E[g_t^2]$. \n",
    "First observe that if we expand the recursive relations of $v_t,$ $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (g_t \\odot g_t)$ can be re-written as\n",
    "a function of previous gradients:\n",
    "$$\n",
    "v_t = \\beta_2^t v_0 + (1-\\beta_2) \\sum_{i=1}^t \\beta_2^{t-i} \\cdot g_i \\odot g_i \\\\\n",
    "= (1-\\beta_2) \\sum_{i=1}^t \\beta_2^{t-i} \\cdot g_i \\odot g_i\n",
    "$$\n",
    "since $v_0 = 0$. Recall that $g_t$ is a random variable, drawn from some probability distribution of gradients. Since $v_t$ is a function of random variable $g_i$s, $v_t$ is a random variable too. Taking the expectation of $v_t$ yields\n",
    "$$\n",
    "E[v_t] = (1 - \\beta_2) E[\\sum_{i=1}^t \\beta_2^{t-i} \\cdot g_i \\odot g_i]  \\\\\n",
    "= (1 - \\beta_2) \\sum_{i=1}^t \\beta_2^{t-i} \\cdot E[g_i \\odot g_i]   \\quad \\text{(Linearity of Expectation)} \n",
    "$$\n",
    "Denote $g_i \\odot g_i$ as $g_i^2$ for convenience. Then,\n",
    "$$\n",
    "E[v_t] = (1 - \\beta_2) \\sum_{i=1}^t \\beta_2^{t-i} \\cdot E[g_i^2] \\\\\n",
    "= (1 - \\beta_2) \\left ( \\beta_2^0 E[g_t^2] + \\beta_2^1 E[g_{t-1}^2] + \\beta_2^2 E[g_{t-2}^2] + \\cdots + \\beta_2^{t-1} E[g_1^2] \\right ) \\\\\n",
    "= 1 \\cdot E[g_t^2] + \\beta_2^1 (E[g_{t-1}^2] - E[g_t^2]) + \\beta_2^2 (E[g_{t-2}^2]  - E[g_{t-1}^2]) + \\cdots + \\beta_2^{t-1} (E[g_1^2] - E[g_2^2]) - \\beta_2^{t} E[g_1^2]   \\\\\n",
    "\\approx ( 1 - \\beta_2^t) \\cdot E[g_t^2]\n",
    "$$\n",
    "Therefore, the algorithm divides $v_t$ by this term $(1 - \\beta_2^t)$ to get a bias-corrected version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\beta_2^t$ will be decreased exponentially as $t$ becomes large. This means that dividing $v_t$ by $1 - \\beta_2^t$ will have an effect only in the initial epochs, where $t$ is small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"memory\" of the algorithm is controlled by $\\beta_2$. Intuitively, the average number of epochs that the algorithm remembers updates is approximately $\\frac{1}{1 - \\beta_2}$. For example, with the default value of $\\beta_2 = 0.999$, the algorithm will remember the information of the first update up to epoch 1000 (= 1 / (1 - 0.999)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitions behind Adam's update rules\n",
    "\n",
    "### Adam's update rule \n",
    "$$\n",
    "m_0 = v_0 = 0 \\\\\n",
    "m_t =  \\beta_1 m_{t-1} + (1- \\beta_1) g_t \\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (g_t \\odot g_t) \\\\\n",
    "\\hat{m_t} = m_t (bias-corrected)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\odot$ is the element-wise product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35min + 1:30min + 40min (reading the paper included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "http://exochronos.hatenablog.com/entry/2016/05/07/194033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
